system:
  name: "sundaygraph"
  version: "1.0.0"
  log_level: "INFO"
  log_file: "logs/sundaygraph.log"

data:
  input_dir: "./data/input"
  output_dir: "./data/output"
  cache_dir: "./data/cache"
  supported_formats:
    - json
    - csv
    - txt
    - xml
    - pdf
    - docx
  max_file_size_mb: 100

ontology:
  schema_path: "./config/ontology_schema.yaml"  # Initial/fallback schema
  auto_validate: true
  strict_mode: false
  allow_custom_properties: true
  build_with_llm: true  # Build schema using LLM reasoning
  store_in_postgres: true  # Store schema metadata in PostgreSQL
  evolve_automatically: true  # Evolve schema based on data
  enable_evaluation: true  # Enable quality evaluation metrics

graph:
  backend: "oxigraph"  # "memory" or "oxigraph" - Using Oxigraph in Docker
  memory:
    directed: true
    multigraph: false
  oxigraph:
    sparql_endpoint: "http://oxigraph:7878/query"  # Docker service name
    update_endpoint: "http://oxigraph:7878/update"  # Docker service name
    default_graph_uri: "http://sundaygraph.org/graph"  # Default graph URI
    timeout: 30  # Request timeout in seconds

# PostgreSQL for schema metadata storage (OntoCast-inspired)
schema_store:
  enabled: true
  # Use service name in Docker, localhost for local
  host: "postgres"  # Docker service name
  port: 5432
  database: "sundaygraph"
  user: "postgres"
  password: "password"

agents:
  data_ingestion:
    enabled: true
    batch_size: 100
    max_workers: 4
    chunk_size: 1000
    overlap: 200
    extract_entities: true
    extract_relations: true
    
  ontology:
    enabled: true
    strict_mode: false
    auto_map_properties: true
    validation_level: "medium"  # Options: "strict", "medium", "loose"
    use_llm_reasoning: true  # Use LLM for intelligent reasoning
    
  graph_construction:
    enabled: true
    batch_insert_size: 1000
    create_indexes: true
    deduplicate_entities: true
    merge_relations: true
    
  query:
    enabled: true
    max_results: 100
    similarity_threshold: 0.7
    use_semantic_search: true

processing:
  nlp:
    model: "en_core_web_sm"
    use_gpu: false
    batch_size: 32
    
  embedding:
    model: "sentence-transformers/all-MiniLM-L6-v2"
    dimension: 384
    device: "cpu"
    
  llm:
    provider: "openai"  # Using OpenAI for all reasoning
    model: "gpt-4"  # Will auto-select cheaper models for simple tasks
    temperature: 0.7
    max_tokens: 2000
    api_key: "${OPENAI_API_KEY}"  # From environment variable
    enable_cache: true  # Enable response caching to reduce costs
    cache_ttl: 3600  # Cache time-to-live in seconds (1 hour)

storage:
  persist_graph: true
  graph_file: "./data/graph.pkl"
  backup_enabled: true
  backup_interval_hours: 24
